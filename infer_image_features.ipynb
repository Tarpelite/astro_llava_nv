{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.mllama.modeling_mllama import MllamaVisionModel\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "from astropy.table import Table\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21051,) <class 'astropy.table.column.Column'>\n"
     ]
    }
   ],
   "source": [
    "# read all csv table\n",
    "csv_data = Table.read(\"/mnt/data/CVPR2025/task1_data/test_no_classification.hdf5\")\n",
    "csv_data.keys()\n",
    "print(csv_data[\"TARGETID\"].shape, type(csv_data[\"TARGETID\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e9e8c501f54236a8939975901045a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load a vision model\n",
    "checkpoint=\"/mnt/data/CVPR2025/task1_data/Llama-3.2-11B-Vision-Instruct\"\n",
    "model = MllamaVisionModel.from_pretrained(checkpoint)\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MllamaVisionModel(\n",
       "  (patch_embedding): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14), padding=valid, bias=False)\n",
       "  (gated_positional_embedding): MllamaPrecomputedPositionEmbedding(\n",
       "    (tile_embedding): Embedding(9, 8197120)\n",
       "  )\n",
       "  (pre_tile_positional_embedding): MllamaPrecomputedAspectRatioEmbedding(\n",
       "    (embedding): Embedding(9, 5120)\n",
       "  )\n",
       "  (post_tile_positional_embedding): MllamaPrecomputedAspectRatioEmbedding(\n",
       "    (embedding): Embedding(9, 5120)\n",
       "  )\n",
       "  (layernorm_pre): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  (layernorm_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  (transformer): MllamaVisionEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MllamaVisionEncoderLayer(\n",
       "        (self_attn): MllamaVisionSdpaAttention(\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (o_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (mlp): MllamaVisionMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (global_transformer): MllamaVisionEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x MllamaVisionEncoderLayer(\n",
       "        (self_attn): MllamaVisionSdpaAttention(\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (o_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (mlp): MllamaVisionMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aacc420f1db5410aa0a639abdaa37a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21051 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_root_dir = \"/mnt/data/CVPR2025/task1_data/images/images\"\n",
    "all_image_paths = []\n",
    "for id_ in tqdm(csv_data[\"TARGETID\"]):\n",
    "    img_path = os.path.join(image_root_dir, \"{}.png\".format(id_))\n",
    "    all_image_paths.append(img_path)\n",
    "    # inputs = processor(images=test_image, return_tensors=\"pt\")\n",
    "    # with torch.no_grad():\n",
    "    #     for k in inputs:\n",
    "    #         inputs[k] = inputs[k].cuda()\n",
    "    #     output = model(**inputs)\n",
    "    #     all_features.append(output[0].cpu().squeeze(0).squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mnt/data/CVPR2025/task1_data/images/images/39632936139492141.png',\n",
       " '/mnt/data/CVPR2025/task1_data/images/images/39632936139492779.png',\n",
       " '/mnt/data/CVPR2025/task1_data/images/images/39632936143685968.png',\n",
       " '/mnt/data/CVPR2025/task1_data/images/images/39632936143686134.png']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_image_paths[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 4, 1601, 7680])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# test image inputs\n",
    "test_image_paths = all_image_paths[:8]\n",
    "test_images = [Image.open(x) for x in test_image_paths]\n",
    "inputs = processor(images=test_images, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    for k in inputs:\n",
    "        inputs[k] = inputs[k].cuda()\n",
    "    output = model(**inputs)\n",
    "print(output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/2632 [00:10<1:34:50,  2.17s/it]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 初始化存储所有特征的列表\n",
    "all_image_features = []\n",
    "\n",
    "# 设置批次大小\n",
    "batch_size = 8\n",
    "\n",
    "# 将模型转换为 bf16\n",
    "model = model.bfloat16()\n",
    "\n",
    "# 计算需要处理的批次数\n",
    "num_batches = len(all_image_paths) // batch_size\n",
    "if len(all_image_paths) % batch_size != 0:\n",
    "    num_batches += 1\n",
    "\n",
    "# 使用tqdm显示处理进度\n",
    "for i in tqdm(range(num_batches)):\n",
    "    # 获取当前批次的图像路径\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(all_image_paths))\n",
    "    batch_image_paths = all_image_paths[start_idx:end_idx]\n",
    "    \n",
    "    # 读取并处理图像\n",
    "    batch_images = [Image.open(x) for x in batch_image_paths]\n",
    "    inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
    "    \n",
    "    # 将输入移到GPU\n",
    "    with torch.no_grad():\n",
    "        for k in inputs:\n",
    "            inputs[k] = inputs[k].cuda()\n",
    "        \n",
    "        # 模型推理\n",
    "        output = model(**inputs)\n",
    "        \n",
    "        # 将输出移到CPU并转换为numpy数组\n",
    "        features = output[0].cpu().float().squeeze(0).numpy()\n",
    "        all_image_features.append(features)\n",
    "\n",
    "# 合并所有特征\n",
    "all_image_features = np.concatenate(all_image_features, axis=1)\n",
    "\n",
    "print(\"处理完成!\")\n",
    "print(f\"特征形状: {all_image_features.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
