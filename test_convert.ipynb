{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import MllamaConfig, MllamaForConditionalGeneration\n",
    "from transformers.models.mllama.configuration_mllama import MllamaTextConfig,MllamaVisionConfig\n",
    "from transformers.models.mllama.modeling_mllama import MllamaCrossAttentionDecoderLayer\n",
    "from transformers.utils import logging\n",
    "from transformers.modeling_rope_utils import rope_config_validation\n",
    "from typing import Dict, List, Optional, Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AstrollavaTextConfig(MllamaTextConfig):\n",
    "    model_type = \"mllama_text_model\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 128256,\n",
    "        hidden_size: int = 4096,\n",
    "        hidden_act: str = \"silu\",\n",
    "        num_hidden_layers: int = 40,\n",
    "        num_attention_heads: int = 32,\n",
    "        num_key_value_heads: int = 8,\n",
    "        intermediate_size: int = 14_336,\n",
    "        structure_output_dim: int=1024,\n",
    "        spectrum_output_dim: int=1024,\n",
    "        rope_theta: float = 500_000,\n",
    "        rope_scaling: Optional[Dict] = None,\n",
    "        rms_norm_eps: float = 1e-5,\n",
    "        max_position_embeddings: int = 131_072,\n",
    "        initializer_range: float = 0.02,\n",
    "        use_cache: bool = True,\n",
    "        tie_word_embeddings: bool = False,\n",
    "        cross_attention_layers: Optional[List[int]] = None,\n",
    "        structure_cross_attention_layers: Optional[List[int]] = None,\n",
    "        spectrum_cross_attention_layers: Optional[List[int]] = None,\n",
    "        vision_cross_attention_layers:Optional[List[int]] = None,\n",
    "        dropout: float = 0,\n",
    "        bos_token_id: int = 128000,\n",
    "        eos_token_id: int = 128001,\n",
    "        pad_token_id: Optional[int] = 128004,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if cross_attention_layers is None:\n",
    "            cross_attention_layers = [3, 8, 13, 18, 23, 28, 33, 38]\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.cross_attention_layers = cross_attention_layers\n",
    "        # add cross attn for graph structure and spectrum encoders\n",
    "        self.structure_cross_attention_layers = structure_cross_attention_layers\n",
    "        self.spectrum_cross_attention_layers = spectrum_cross_attention_layers \n",
    "        self.vision_cross_attention_layers = vision_cross_attention_layers\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.structure_output_dim = structure_output_dim\n",
    "        self.spectrum_output_dim = spectrum_output_dim\n",
    "\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.initializer_range = initializer_range\n",
    "        self.use_cache = use_cache\n",
    "        self.rope_theta = rope_theta\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.dropout = dropout\n",
    "        self.hidden_act = hidden_act\n",
    "        self.rope_scaling = rope_scaling\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        rope_config_validation(self)\n",
    "\n",
    "        super().__init__(\n",
    "            pad_token_id=pad_token_id,\n",
    "            bos_token_id=bos_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "            tie_word_embeddings=tie_word_embeddings,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n",
    "        cls._set_token_in_kwargs(kwargs)\n",
    "\n",
    "        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
    "\n",
    "        if config_dict.get(\"model_type\") == \"mllama\":\n",
    "            config_dict = config_dict[\"text_config\"]\n",
    "\n",
    "        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n",
    "           print(\n",
    "                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n",
    "                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n",
    "            )\n",
    "\n",
    "        return cls.from_dict(config_dict, **kwargs)\n",
    "\n",
    "\n",
    "class AstrollavaConfig(MllamaConfig):\n",
    "    model_type = \"mllama\"\n",
    "    is_composition = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_config=None,\n",
    "        text_config=None,\n",
    "        image_token_index=128256,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if vision_config is None:\n",
    "            self.vision_config = MllamaVisionConfig()\n",
    "            print(\"vision_config is None, using default mllama vision config\")\n",
    "        elif isinstance(vision_config, dict):\n",
    "            self.vision_config = MllamaVisionConfig(**vision_config)\n",
    "        elif isinstance(vision_config, MllamaVisionConfig):\n",
    "            self.vision_config = vision_config\n",
    "\n",
    "        self.image_token_index = image_token_index\n",
    "\n",
    "        if text_config is None:\n",
    "            self.text_config = AstrollavaTextConfig()\n",
    "            print(\"text_config is None, using default mllama text config\")\n",
    "        elif isinstance(text_config, dict):\n",
    "            self.text_config = AstrollavaTextConfig(**text_config)\n",
    "        elif isinstance(text_config, AstrollavaTextConfig):\n",
    "            self.text_config = text_config\n",
    "\n",
    "        super().__init__(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Model class\n",
    "import torch.nn as nn\n",
    "from transformers.models.mllama.modeling_mllama import MllamaPreTrainedModel, MllamaSelfAttentionDecoderLayer, MllamaTextRMSNorm, MllamaRotaryEmbedding, _prepare_4d_causal_attention_mask_with_cache_position\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPast, CausalLMOutputWithPast\n",
    "from transformers.cache_utils import Cache\n",
    "from transformers.modeling_attn_mask_utils import AttentionMaskConverter\n",
    "\n",
    "\n",
    "class AstroMllamaTextModel(MllamaPreTrainedModel):\n",
    "    config_class = MllamaTextConfig\n",
    "    base_model_prefix = \"language_model.model\"\n",
    "\n",
    "    def __init__(self, config: MllamaTextConfig):\n",
    "        super().__init__(config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size + 8, config.hidden_size, self.padding_idx)\n",
    "        self.cross_attention_layers = config.cross_attention_layers\n",
    "        self.structure_cross_attention_layers = config.structure_cross_attention_layers\n",
    "        self.spectrum_cross_attention_layers = config.spectrum_cross_attention_layers\n",
    "        self.vision_cross_attention_layers =  config.vision_cross_attention_layers\n",
    "\n",
    "\n",
    "        layers = []\n",
    "        for layer_idx in range(config.num_hidden_layers):\n",
    "            if layer_idx in self.cross_attention_layers:\n",
    "                layers.append(MllamaCrossAttentionDecoderLayer(config, layer_idx))\n",
    "            else:\n",
    "                layers.append(MllamaSelfAttentionDecoderLayer(config, layer_idx))\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = MllamaTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.rotary_emb = MllamaRotaryEmbedding(config=config)\n",
    "        self.gradient_checkpointing = False\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embed_tokens = value\n",
    "\n",
    "    # @add_start_docstrings_to_model_forward(MLLAMA_TEXT_INPUTS_DOCSTRING)\n",
    "    # @replace_return_docstrings(output_type=BaseModelOutputWithPast, config_class=\"MllamaTextConfig\")\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        # cross_attention_states: Optional[torch.FloatTensor] = None,\n",
    "        # cross_attention_mask: Optional[torch.Tensor] = None,\n",
    "        strurcture_attention_states: Optional[torch.FloatTensor] = None,\n",
    "        structure_attention_mask: Optional[torch.Tensor] = None,\n",
    "        spectrum_attention_states:  Optional[torch.FloatTensor] = None,\n",
    "        spectrum_attention_mask: Optional[torch.Tensor] = None,\n",
    "        vision_attention_states:  Optional[torch.FloatTensor] = None,\n",
    "        vision_attention_mask: Optional[torch.Tensor] = None,\n",
    "        full_text_row_masked_out_mask: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "        \"\"\"\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoProcessor, MllamaTextModel\n",
    "\n",
    "        >>> checkpoint = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "        >>> model = MllamaTextModel.from_pretrained(checkpoint)\n",
    "        >>> processor = AutoProcessor.from_pretrained(checkpoint)\n",
    "\n",
    "        >>> text = \"<|image|>If I had to write a haiku for this one\"\n",
    "        >>> inputs = processor(text=text, return_tensors=\"pt\")\n",
    "\n",
    "        >>> output = model(**inputs)\n",
    "\n",
    "        >>> print(output.last_hidden_state.shape)\n",
    "        torch.Size([1, 13, 4096])\n",
    "        ```\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n",
    "            )\n",
    "\n",
    "        if self.gradient_checkpointing and self.training and use_cache:\n",
    "            print(\n",
    "                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n",
    "            )\n",
    "            use_cache = False\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        causal_mask = self._update_causal_mask(\n",
    "            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "        )\n",
    "\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = None\n",
    "\n",
    "        for idx, decoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            # For text-only path we should skip cross attention layers.\n",
    "            # Let's check if the layer is cross attention layer and if we have cross attention states\n",
    "            # or cached cross attention states.\n",
    "            is_cross_attention_layer = idx in self.cross_attention_layers\n",
    "            is_cross_attention_cache_empty = past_key_values is None or (\n",
    "                past_key_values is not None and past_key_values.get_seq_length(idx) == 0\n",
    "            )\n",
    "            # By tianyu ,control the cross attention states flow in \n",
    "            # [Start]\n",
    "            if idx in self.structure_cross_attention_layers:\n",
    "                is_structure_cross_attention_layer = True\n",
    "                cross_attention_states = strurcture_attention_states\n",
    "                cross_attention_mask = structure_attention_mask\n",
    "            elif idx in self.spectrum_cross_attention_layers:\n",
    "                is_spectrum_cross_attention_layer = True\n",
    "                cross_attention_states = spectrum_attention_states\n",
    "                cross_attention_mask = spectrum_attention_mask\n",
    "            elif idx in self.vision_cross_attention_layers:\n",
    "                is_vision_cross_attention_layer = True\n",
    "                cross_attention_states = vision_attention_states\n",
    "                cross_attention_mask = vision_attention_mask\n",
    "            # [End]\n",
    "            \n",
    "            if is_cross_attention_layer and cross_attention_states is None and is_cross_attention_cache_empty:\n",
    "                continue\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    decoder_layer.__call__,\n",
    "                    hidden_states,\n",
    "                    cross_attention_states,\n",
    "                    cross_attention_mask,\n",
    "                    causal_mask,\n",
    "                    full_text_row_masked_out_mask,\n",
    "                    position_ids,\n",
    "                    past_key_values,\n",
    "                    output_attentions,\n",
    "                    use_cache,\n",
    "                    cache_position,\n",
    "                    position_embeddings,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    cross_attention_states=cross_attention_states,\n",
    "                    cross_attention_mask=cross_attention_mask,\n",
    "                    attention_mask=causal_mask,\n",
    "                    full_text_row_masked_out_mask=full_text_row_masked_out_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_values,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                    cache_position=cache_position,\n",
    "                    position_embeddings=position_embeddings,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = next_decoder_cache if use_cache else None\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "        )\n",
    "\n",
    "    def _update_causal_mask(\n",
    "        self,\n",
    "        attention_mask: torch.Tensor,\n",
    "        input_tensor: torch.Tensor,\n",
    "        cache_position: torch.Tensor,\n",
    "        past_key_values: Cache,\n",
    "        output_attentions: bool,\n",
    "    ):\n",
    "        if self.config._attn_implementation == \"flash_attention_2\":\n",
    "            if attention_mask is not None and 0.0 in attention_mask:\n",
    "                return attention_mask\n",
    "            return None\n",
    "\n",
    "        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n",
    "        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n",
    "        # to infer the attention mask.\n",
    "        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "\n",
    "        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n",
    "        # TODO: we have only SDPA currently and there's a bug when attn-bias is passed. Need to add eager attn and return the line\n",
    "        # self.config._attn_implementation == \"sdpa\" and\n",
    "        if self.config._attn_implementation == \"sdpa\" and not output_attentions:\n",
    "            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n",
    "                attention_mask,\n",
    "                inputs_embeds=input_tensor,\n",
    "                past_key_values_length=past_seen_tokens,\n",
    "                is_training=self.training,\n",
    "            ):\n",
    "                return None\n",
    "\n",
    "        dtype, device = input_tensor.dtype, input_tensor.device\n",
    "        min_dtype = torch.finfo(dtype).min\n",
    "        sequence_length = input_tensor.shape[1]\n",
    "        target_length = (\n",
    "            attention_mask.shape[-1]\n",
    "            if isinstance(attention_mask, torch.Tensor)\n",
    "            else past_seen_tokens + sequence_length + 1\n",
    "        )\n",
    "\n",
    "        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n",
    "        causal_mask = _prepare_4d_causal_attention_mask_with_cache_position(\n",
    "            attention_mask,\n",
    "            sequence_length=sequence_length,\n",
    "            target_length=target_length,\n",
    "            dtype=dtype,\n",
    "            device=device,\n",
    "            min_dtype=min_dtype,\n",
    "            cache_position=cache_position,\n",
    "            batch_size=input_tensor.shape[0],\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            self.config._attn_implementation == \"sdpa\"\n",
    "            and attention_mask is not None\n",
    "            and attention_mask.device.type == \"cuda\"\n",
    "            and not output_attentions\n",
    "        ):\n",
    "            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n",
    "            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n",
    "            # Details: https://github.com/pytorch/pytorch/issues/110213\n",
    "            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n",
    "\n",
    "        return causal_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation import GenerationMixin\n",
    "from transformers.models.mllama.modeling_mllama import MllamaVisionModel,_prepare_cross_attention_mask\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from .models import SpecFormer\n",
    "\n",
    "class AstroMllamaForCausalLM(MllamaPreTrainedModel, GenerationMixin):\n",
    "    config_class = MllamaTextConfig\n",
    "    base_model_prefix = \"language_model\"\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config.get_text_config())\n",
    "        self.text_config = config.get_text_config()\n",
    "        self.vocab_size = self.text_config.vocab_size\n",
    "        self.model = AstroMllamaTextModel._from_config(self.text_config, attn_implementation=config._attn_implementation)\n",
    "        self.lm_head = nn.Linear(self.text_config.hidden_size, self.vocab_size, bias=False)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.embed_tokens = value\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def set_decoder(self, decoder):\n",
    "        self.model = decoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.model\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "         # cross_attention_states: Optional[torch.FloatTensor] = None,\n",
    "        # cross_attention_mask: Optional[torch.Tensor] = None,\n",
    "        strurcture_attention_states: Optional[torch.FloatTensor] = None,\n",
    "        structure_attention_mask: Optional[torch.Tensor] = None,\n",
    "        spectrum_attention_states:  Optional[torch.FloatTensor] = None,\n",
    "        spectrum_attention_mask: Optional[torch.Tensor] = None,\n",
    "        vision_attention_states:  Optional[torch.FloatTensor] = None,\n",
    "        vision_attention_mask: Optional[torch.Tensor] = None,\n",
    "        full_text_row_masked_out_mask: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        num_logits_to_keep: int = 0,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "            num_logits_to_keep (`int`, *optional*):\n",
    "                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n",
    "                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n",
    "                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, MllamaForCausalLM\n",
    "\n",
    "        >>> model = MllamaForCausalLM.from_pretrained(\"Llama-3.2-11B-Vision\")\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"Llama-3.2-11B-Vision\")\n",
    "\n",
    "        >>> prompt = \"If I had to write a haiku, it would be:\"\n",
    "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=40, do_sample=True, temperature=0.6)\n",
    "        >>> result = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        >>> print(result)\n",
    "        If I had to write a haiku, it would be: \"Snowflakes gently fall\" - simple, yet peaceful.\n",
    "        I love the idea of snowflakes gently falling, each one\n",
    "        ```\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            # cross_attention_states=cross_attention_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            # cross_attention_mask=cross_attention_mask,\n",
    "            strurcture_attention_states = strurcture_attention_states,\n",
    "            structure_attention_mask = structure_attention_mask,\n",
    "            spectrum_attention_states = spectrum_attention_states,\n",
    "            spectrum_attention_mask = spectrum_attention_mask,\n",
    "            vision_attention_states = vision_attention_states,\n",
    "            vision_attention_mask = vision_attention_mask,\n",
    "            full_text_row_masked_out_mask=full_text_row_masked_out_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Upcast to float if we need to compute the loss to avoid potential precision issues\n",
    "            logits = logits.float()\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        input_ids,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        cache_position=None,\n",
    "        position_ids=None,\n",
    "        use_cache=True,\n",
    "        num_logits_to_keep=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n",
    "        # Exception 1: when passing input_embeds, input_ids may be missing entries\n",
    "        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n",
    "        if past_key_values is not None:\n",
    "            if inputs_embeds is not None:  # Exception 1\n",
    "                input_ids = input_ids[:, -cache_position.shape[0] :]\n",
    "            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n",
    "                input_ids = input_ids[:, cache_position]\n",
    "\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past_key_values:\n",
    "                position_ids = position_ids[:, -input_ids.shape[1] :]\n",
    "\n",
    "                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n",
    "                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n",
    "\n",
    "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
    "        if inputs_embeds is not None and cache_position[0] == 0:\n",
    "            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n",
    "        else:\n",
    "            # The clone here is for the same reason as for `position_ids`.\n",
    "            model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n",
    "\n",
    "        if num_logits_to_keep is not None:\n",
    "            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n",
    "\n",
    "        model_inputs.update(\n",
    "            {\n",
    "                \"position_ids\": position_ids,\n",
    "                \"cache_position\": cache_position,\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"use_cache\": use_cache,\n",
    "                \"attention_mask\": attention_mask,\n",
    "            }\n",
    "        )\n",
    "        return model_inputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AstroMllamaForConditionalGeneration(MllamaPreTrainedModel, GenerationMixin):\n",
    "    def __init__(self, config: AstrollavaConfig):\n",
    "        super().__init__(config)\n",
    "        self.vocab_size = config.text_config.vocab_size\n",
    "        self.hidden_size = config.text_config.hidden_size\n",
    "        self.max_num_tiles = config.vision_config.max_num_tiles\n",
    "        self.vision_output_dim = config.vision_config.vision_output_dim\n",
    "        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n",
    "\n",
    "        self.vision_model = MllamaVisionModel._from_config(\n",
    "            config.vision_config, attn_implementation=config._attn_implementation\n",
    "        )\n",
    "        self.language_model = AstroMllamaForCausalLM._from_config(\n",
    "            config.text_config, attn_implementation=config._attn_implementation\n",
    "        )\n",
    "\n",
    "        self.spec_model = \n",
    "\n",
    "        # structure modal\n",
    "        self.structure_modal_projector = nn.Linear(\n",
    "            config.text_config.structure_output_dim,\n",
    "            config.text_config.hidden_size,\n",
    "            bias = True,\n",
    "        )\n",
    "\n",
    "        # spectrum modal\n",
    "        self.spectrum_modal_projector =  nn.Linear(\n",
    "            config.text_config.spectrum_output_dim,\n",
    "            config.text_config.hidden_size,\n",
    "            bias = True,\n",
    "        )\n",
    "\n",
    "        # vision modal\n",
    "        self.multi_modal_projector = nn.Linear(\n",
    "            config.vision_config.vision_output_dim,\n",
    "            config.text_config.hidden_size,\n",
    "            bias=True,\n",
    "        )\n",
    "       \n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.language_model.get_input_embeddings()\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.language_model.set_input_embeddings(value)\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.language_model.get_output_embeddings()\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.language_model.set_output_embeddings(new_embeddings)\n",
    "\n",
    "    def set_decoder(self, decoder):\n",
    "        self.language_model.set_decoder(decoder)\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.language_model.get_decoder()\n",
    "\n",
    "    def tie_weights(self):\n",
    "        return self.language_model.tie_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        aspect_ratio_mask: Optional[torch.Tensor] = None,\n",
    "        aspect_ratio_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        # cross_attention_states: Optional[torch.FloatTensor] = None,\n",
    "        # cross_attention_mask: Optional[torch.Tensor] = None,\n",
    "        strurcture_attention_states: Optional[torch.FloatTensor] = None,\n",
    "        structure_attention_mask: Optional[torch.Tensor] = None,\n",
    "        spectrum_attention_states:  Optional[torch.FloatTensor] = None,\n",
    "        spectrum_attention_mask: Optional[torch.Tensor] = None,\n",
    "       \n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        num_logits_to_keep: int = 0,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "            num_logits_to_keep (`int`, *optional*):\n",
    "                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n",
    "                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n",
    "                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, MllamaForConditionalGeneration\n",
    "\n",
    "        >>> checkpoint = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "        >>> model = MllamaForConditionalGeneration.from_pretrained(checkpoint)\n",
    "        >>> processor = AutoProcessor.from_pretrained(checkpoint)\n",
    "\n",
    "        >>> prompt = \"<|image|>If I had to write a haiku for this one\"\n",
    "        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        >>> inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> output = model.generate(**inputs, max_new_tokens=15)\n",
    "\n",
    "        >>> prompt_len = inputs.input_ids.shape[-1]\n",
    "        >>> generated_ids = output[:, prompt_len:]\n",
    "        >>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        >>> print(generated_text)\n",
    "        [', it would be:.\\\\nA stop sign in Chinatown.\\\\n']\n",
    "        ```\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n",
    "            )\n",
    "\n",
    "        if pixel_values is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n",
    "            )\n",
    "\n",
    "        if pixel_values is not None and cross_attention_states is not None:\n",
    "            raise ValueError(\"`pixel_values` and `cross_attention_states` cannot be provided simultaneously\")\n",
    "\n",
    "        if pixel_values is not None:\n",
    "            if aspect_ratio_ids is None:\n",
    "                raise ValueError(\"`aspect_ratio_ids` must be provided if `pixel_values` is provided\")\n",
    "            \n",
    "            # get vision tokens from vision model\n",
    "            vision_outputs = self.vision_model(\n",
    "                pixel_values=pixel_values,\n",
    "                aspect_ratio_ids=aspect_ratio_ids,\n",
    "                aspect_ratio_mask=aspect_ratio_mask,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                output_attentions=output_attentions,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "            cross_attention_states = vision_outputs[0]\n",
    "            cross_attention_states = self.multi_modal_projector(cross_attention_states).reshape(\n",
    "                -1, cross_attention_states.shape[-2], self.hidden_size\n",
    "            )\n",
    "\n",
    "        if cross_attention_mask is not None:\n",
    "            cross_attention_mask, full_text_row_masked_out_mask = _prepare_cross_attention_mask(\n",
    "                cross_attention_mask,\n",
    "                num_vision_tokens=self.vision_model.num_patches,\n",
    "                dtype=self.dtype,\n",
    "            )\n",
    "        else:\n",
    "            full_text_row_masked_out_mask = None\n",
    "\n",
    "        if cross_attention_mask is not None and cache_position is not None:\n",
    "            cross_attention_mask = cross_attention_mask[:, :, cache_position]\n",
    "            full_text_row_masked_out_mask = full_text_row_masked_out_mask[:, :, cache_position]\n",
    "        \n",
    "\n",
    "\n",
    "        outputs = self.language_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            # cross_attention_states=cross_attention_states,\n",
    "            # cross_attention_mask=cross_attention_mask,\n",
    "            strurcture_attention_states = \n",
    "            full_text_row_masked_out_mask=full_text_row_masked_out_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            labels=labels,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            output_attentions=output_attentions,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "            num_logits_to_keep=num_logits_to_keep,\n",
    "        )\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        inputs_embeds=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        pixel_values=None,\n",
    "        aspect_ratio_ids=None,\n",
    "        aspect_ratio_mask=None,\n",
    "        cross_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=False,\n",
    "        cache_position=None,\n",
    "        num_logits_to_keep=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n",
    "        # Exception 1: when passing input_embeds, input_ids may be missing entries\n",
    "        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n",
    "        if past_key_values is not None:\n",
    "            if inputs_embeds is not None:  # Exception 1\n",
    "                input_ids = input_ids[:, -cache_position.shape[0] :]\n",
    "            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n",
    "                input_ids = input_ids[:, cache_position]\n",
    "\n",
    "        # TODO: we have no attention_mask so this won't work, check if we really won't need attention mask and find another way\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past_key_values:\n",
    "                position_ids = position_ids[:, -input_ids.shape[1] :]\n",
    "\n",
    "                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n",
    "                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n",
    "\n",
    "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
    "        if inputs_embeds is not None and cache_position[0] == 0:\n",
    "            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n",
    "        else:\n",
    "            # The clone here is for the same reason as for `position_ids`.\n",
    "            model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n",
    "\n",
    "        if num_logits_to_keep is not None:\n",
    "            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n",
    "\n",
    "        model_inputs.update(\n",
    "            {\n",
    "                \"position_ids\": position_ids,\n",
    "                \"cache_position\": cache_position,\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"use_cache\": use_cache,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"cross_attention_mask\": cross_attention_mask,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # If we're in pre-fill or cacheless decoding step, then we need pixel_values and aspect ratios\n",
    "        # to compute image hidden states, otherwise they are cached within each cross attn layer\n",
    "        if (input_ids == self.config.image_token_index).any():\n",
    "            model_inputs[\"pixel_values\"] = pixel_values\n",
    "            model_inputs[\"aspect_ratio_ids\"] = aspect_ratio_ids\n",
    "            model_inputs[\"aspect_ratio_mask\"] = aspect_ratio_mask\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    def _update_model_kwargs_for_generation(self, outputs, model_kwargs, is_encoder_decoder, **kwargs):\n",
    "        cross_attention_mask_prev = model_kwargs.get(\"cross_attention_mask\", None)\n",
    "        model_kwargs = super()._update_model_kwargs_for_generation(\n",
    "            outputs=outputs,\n",
    "            model_kwargs=model_kwargs,\n",
    "            is_encoder_decoder=is_encoder_decoder,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # add cross-attn mask for new token\n",
    "        if cross_attention_mask_prev is not None:\n",
    "            model_kwargs[\"cross_attention_mask\"] = torch.cat(\n",
    "                [cross_attention_mask_prev, cross_attention_mask_prev[:, -1:, ...]], dim=1\n",
    "            )\n",
    "        return model_kwargs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_config(original_config):\n",
    "    new_config = AstrollavaConfig()\n",
    "    \n",
    "    # 为每个原始cross attention层添加两个新层\n",
    "    new_config.text_config.cross_attention_layers = [\n",
    "        layer for layer in original_config.text_config.cross_attention_layers\n",
    "        for _ in range(3)\n",
    "    ]\n",
    "    \n",
    "    return new_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_config = Astrollavaconfig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
